{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l369kWshSuaY"
      },
      "source": [
        "# 11. 다대다 RNN을 이용한 텍스트 생성\n",
        "\n",
        "이번 챕터에서는 다대다 RNN을 이용하여 텍스트 생성을 해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1Q1WCJ9S_Vb"
      },
      "source": [
        "# 1. 문자 단위 RNN(Char RNN)\n",
        "\n",
        "이번 챕터에서는 모든 시점의 입력에 대해서 모든 시점에 대해서 출력을 하는 다대다 RNN을 구현해봅시다. 다대다 RNN은 대표적으로 품사 태깅, 개채명 인식 등에서 사용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rVDgYTDTbqQ"
      },
      "source": [
        "## 1.1 문자 단위 RNN(Char RNN)\n",
        "\n",
        "RNN의 입출력의 단위가 단어 레벨(word-level)이 아니라 문자 레벨(character-level)로 하여 RNN을 구현한다면, 이를 문자 단위 RNN이라고 합니다. RNN 구조 자체가 달라진 것은 아니고, 입, 출력의 단위가 문자로 바뀌었을 뿐입니다. 문자 단위 RNN을 다대다 구조로 구현해봅시다.\n",
        "\n",
        "우선 필요한 도구들을 임포트합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9MBprvlfS-aB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnpFtm0DTjis"
      },
      "source": [
        "### 1. 훈련 데이터 전처리하기\n",
        "\n",
        "여기서는 문자 시퀀스 apple을 입력받으면 pple!를 출력하는 RNN을 구현해볼 겁니다. 이렇게 구현하는 어떤 의미가 있지는 않습니다. 그저 RNN의 동작을 이해하기 위한 목적입니다.\n",
        "\n",
        "입력 데이터와 레이블 데이터에 대해서 문자 집합(voabulary)을 만듭니다. 여기서 문자 집합은 중복을 제거한 문자들의 집합입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QP_HxSETjCq",
        "outputId": "a0240ae7-91f5-4346-965b-20b6ab6e7cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문자 집합의 크기 : 5\n"
          ]
        }
      ],
      "source": [
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print ('문자 집합의 크기 : {}'.format(vocab_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25OBRhkvTxW1"
      },
      "source": [
        "현재 문자 집합에는 총 5개의 문자가 있습니다. !, a, e, l, p입니다. 이제 하이퍼파라미터를 정의해줍니다. 이때 입력은 원-핫 벡터를 사용할 것이므로 입력의 크기는 문자 집합의 크기여야만 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uos3GKVsTzl2"
      },
      "outputs": [],
      "source": [
        "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-VRItuITxYd"
      },
      "source": [
        "이제 문자 집합에 고유한 정수를 부여합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE6zHlbBTwxr",
        "outputId": "cd125c06-7dde-4f8f-f4dc-5ce1404eb3b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ]
        }
      ],
      "source": [
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhq2pLQeT2nW"
      },
      "source": [
        "!은 0, a는 1, e는 2, l은 3, p는 4가 부여되었습니다. 나중에 예측 결과를 다시 문자 시퀀스로 보기위해서 반대로 정수로부터 문자를 얻을 수 있는 `index_to_char`을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR98lLP0T16G",
        "outputId": "f9c0063b-d7fc-4337-a002-ea4ff4ae1e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ]
        }
      ],
      "source": [
        "index_to_char={}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key\n",
        "print(index_to_char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TligTBiJT6fS"
      },
      "source": [
        "이제 입력 데이터와 레이블 데이터의 각 문자들을 정수로 맵핑합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41ctnJVLT4vp",
        "outputId": "c3ee1aed-d0ca-4676-dd3a-bfd68f21aaa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ]
        }
      ],
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o2gkMnCT9q5"
      },
      "source": [
        "파이토치의 `nn.RNN()`은 기본적으로 3차원 텐서를 입력받습니다. 그렇기 때문에 배치 차원을 추가해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEZMvxuZT7iO",
        "outputId": "3020b4a5-4f2b-42e3-9e9a-c0e15aefb812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ]
        }
      ],
      "source": [
        "# 배치 차원 추가\n",
        "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxcqUiDmUA6J"
      },
      "source": [
        "입력 시퀀스의 각 문자들을 원-핫 벡터로 바꿔줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viCC4DbrT_KX",
        "outputId": "e3d2b06e-1744-4a38-a8b9-63486009390b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ]
        }
      ],
      "source": [
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17jYoxp7UDSx"
      },
      "source": [
        "입력 데이터와 레이블 데이터를 텐서로 바꿔줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7AV1EZSKUCRu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1560888/2348034151.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643016022/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  X = torch.FloatTensor(x_one_hot)\n"
          ]
        }
      ],
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rP38u8GUGjw"
      },
      "source": [
        "이제 각 텐서의 크기를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxtqCAwJUE5D",
        "outputId": "0d84f117-ace2-4646-e9be-133468f9d047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
            "레이블의 크기 : torch.Size([1, 5])\n"
          ]
        }
      ],
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE4Opp52UJjj"
      },
      "source": [
        "### 2. 모델 구현하기\n",
        "\n",
        "이제 RNN 모델을 구현해봅시다. 아래에서 fc는 완전 연결층(fully-connected layer)을 의미하며 출력층으로 사용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7aqeLWmiUHu5"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
        "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
        "\n",
        "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB3eYXaAUN-E"
      },
      "source": [
        "클래스로 정의한 모델을 net에 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6Lx0sCNbUPP7"
      },
      "outputs": [],
      "source": [
        "net = Net(input_size, hidden_size, output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5lmwmC8UN_4"
      },
      "source": [
        "이제 입력된 모델에 입력을 넣어서 출력의 크기를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4ep_RsSURxm",
        "outputId": "c4db1160-c3e9-47f3-9f59-a4e3af5b2ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ]
        }
      ],
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sWR0oTvUOB7"
      },
      "source": [
        "(1, 5, 5)의 크기를 가지는데 각각 배치 차원, 시점(timesteps), 출력의 크기입니다. 나중에 정확도를 측정할 때는 이를 모두 펼쳐서 계산하게 되는데, 이때는 view를 사용하여 배치 차원과 시점 차원을 하나로 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHLzpAmbUXRI",
        "outputId": "e8a2edf9-5de4-41b4-e320-0a0075a04be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5, 5])\n"
          ]
        }
      ],
      "source": [
        "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "981TqzNXUOD1"
      },
      "source": [
        "차원이 (5, 5)가 된 것을 볼 수 있습니다. 이제 레이블 데이터의 크기를 다시 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwSpZYXSUaVo",
        "outputId": "132a40d1-571d-4240-dcfa-5747c0ee39f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n"
          ]
        }
      ],
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sTj0MRsUOF-"
      },
      "source": [
        "레이블 데이터는 (1, 5)의 크기를 가지는데, 마찬가지로 나중에 정확도를 측정할 때는 이걸 펼쳐서 계산할 예정입니다. 이 경우 (5)의 크기를 가지게 됩니다. 이제 옵티마이저와 손실 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PnzQO3szUc_B"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF6ITutXUOIC"
      },
      "source": [
        "총 100번의 에포크를 학습합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 1., 0.],\n",
              "         [0., 0., 1., 0., 0.]]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer.zero_grad()\n",
        "outputs = net(X)\n",
        "loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
        "loss.backward() # 기울기 계산\n",
        "optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.3509,  0.4837, -0.3926, -0.0410, -0.0041],\n",
              "         [-0.3653,  0.1640, -0.4751,  0.0952,  0.4664],\n",
              "         [-0.1530,  0.1613, -0.4894,  0.0318,  0.3025],\n",
              "         [-0.3013,  0.5511, -0.0417, -0.5892, -0.1506],\n",
              "         [-0.2938,  0.2738, -0.3581,  0.1850,  0.2613]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWZKN-cuUf9c",
        "outputId": "b406fb0d-00b9-4144-8d61-3b59d2bc19dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 loss:  1.2796375751495361 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "1 loss:  1.0171761512756348 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "2 loss:  0.8011207580566406 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "3 loss:  0.6349550485610962 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "4 loss:  0.48656973242759705 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "5 loss:  0.3621956706047058 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "6 loss:  0.2646353840827942 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "7 loss:  0.19217745959758759 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "8 loss:  0.13888809084892273 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.0980372503399849 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.06706216186285019 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.04593757167458534 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.03338273987174034 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.024486707523465157 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.01698039472103119 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.012035519815981388 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.009161671623587608 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.007345105521380901 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.006062316242605448 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.005096026696264744 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.0043454719707369804 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.0037529864348471165 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.0032800857443362474 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.002899182727560401 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.0025896872393786907 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.0023358813486993313 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.0021260129287838936 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.001950904494151473 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.001803675084374845 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.0016789119690656662 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.0015723875258117914 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.0014806801918894053 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.0014013189356774092 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.0013322115410119295 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.0012716938508674502 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.0012182913487777114 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.0011711004190146923 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.0011290505062788725 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.0010914517333731055 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.0010577093344181776 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.001027299789711833 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.0009996993467211723 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.0009746939758770168 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.0009518553270027041 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0009310167515650392 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.000911820970941335 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.0008941491832956672 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.0008778346818871796 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.0008626871858723462 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.0008486586739309132 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0008355351164937019 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.000823268957901746 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.0008117648540064692 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.0008009512675926089 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0007907808758318424 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.0007811581017449498 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.0007720831199549139 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.0007634606445208192 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.0007552430615760386 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.0007473350269719958 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.0007399033638648689 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.0007326860213652253 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0007258020341396332 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.000719203962944448 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.0007127963472157717 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.0007066745311021805 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.0007006956147961318 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.0006949310773052275 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.000689357053488493 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.0006839497364126146 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.0006786138983443379 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.0006735399947501719 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.0006685137050226331 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.0006636303151026368 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.0006588660180568695 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.0006542208138853312 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.0006496469723060727 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.0006451684748753905 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.0006407852051779628 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.0006364495493471622 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.0006322806584648788 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.0006280878442339599 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.0006240142392925918 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.0006200120551511645 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.0006160096963867545 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.0006121504702605307 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.0006083148764446378 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.0006044793990440667 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0006008105119690299 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.0005970940692350268 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.0005934490473009646 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.000589827832300216 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.0005863256519660354 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.0005828235880471766 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.0005792975425720215 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.0005759383202530444 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.0005725076189264655 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.0005691483383998275 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.0005658367299474776 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.0005625489284284413 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ]
        }
      ],
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
        "    loss.backward() # 기울기 계산\n",
        "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
        "\n",
        "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
        "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBpeG-QKUOKL"
      },
      "source": [
        "# 2. 문자 단위 RNN(Char RNN) - 더 많은 데이터\n",
        "\n",
        "이번 챕터에서는 더 많은 데이터 문자 단위 RNN을 구현합니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYxbhrTeUOMf"
      },
      "source": [
        "## 2.1 문자 단위 RNN(Char RNN)\n",
        "\n",
        "우선 필요한 도구들을 임포트합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WQrXYaDhU0ff"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. 훈련 데이터 전처리하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSP89cDtUOOt"
      },
      "source": [
        "다음과 같이 임의의 샘플을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CXo_XwY2U6Pm"
      },
      "outputs": [],
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5bwyI5LU8S3"
      },
      "source": [
        "문자 집합을 생성하고, 각 문자에 고유한 정수를 부여합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9S2OeOaU9I4",
        "outputId": "54e7b2e4-9151-40f4-9fe7-3d421d7e2576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'k': 0, 't': 1, 's': 2, 'i': 3, 'b': 4, 'a': 5, 'c': 6, ' ': 7, ',': 8, 'd': 9, 'h': 10, 'w': 11, 'p': 12, '.': 13, 'f': 14, 'y': 15, 'u': 16, 'n': 17, 'r': 18, 'o': 19, 'g': 20, 'm': 21, 'l': 22, \"'\": 23, 'e': 24}\n"
          ]
        }
      ],
      "source": [
        "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\n",
        "print(char_dic) # 공백도 여기서는 하나의 원소"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIcHosDJUOQz"
      },
      "source": [
        "각 문자에 정수가 부여되었으며, 총 25개의 문자가 존재합니다. 문자 집합의 크기를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pyuO8nJVBrC",
        "outputId": "78a3e633-5ef4-47a9-8766-6ed4f24203c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문자 집합의 크기 : 25\n"
          ]
        }
      ],
      "source": [
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKm8IQ_qUOS7"
      },
      "source": [
        "문자 집합의 크기는 25이며, 입력을 원-핫 벡터로 사용할 것이므로 이는 매 시점마다 들어갈 입력의 크기이기도 합니다. 이제 하이퍼파라미터를 설정합니다. hidden_size(은닉 상태의 크기)를 입력의 크기와 동일하게 줬는데, 이는 사용자의 선택으로 다른 값을 줘도 무방합니다.\n",
        "\n",
        "그리고 sequence_length라는 변수를 선언했는데, 우리가 앞서 만든 샘플을 10개 단위로 끊어서 샘플을 만들 예정이기 때문입니다. 이는 뒤에서 더 자세히 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VAy83aQnVFMG"
      },
      "outputs": [],
      "source": [
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10  # 임의 숫자 지정\n",
        "learning_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X0h-VtwUOVN"
      },
      "source": [
        "다음은 임의로 지정한 sequence_length 값인 10의 단위로 샘플들을 잘라서 데이터를 만드는 모습을 보여줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_xn0q0bVIBE",
        "outputId": "ca5a58c4-6ffb-49bf-8342-b252f3177a0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ]
        }
      ],
      "source": [
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
        "    y_data.append([char_dic[c] for c in y_str])  # y str to index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWOdIXXqUOXC"
      },
      "source": [
        "총 170개의 샘플이 생성되었습니다. 그리고 각 샘플의 각 문자들은 고유한 정수로 인코딩이 된 상태입니다. 첫번째 샘플의 입력 데이터와 레이블 데이터를 출력해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTOmUtg5VOBp",
        "outputId": "70d040b1-0e75-4de4-8f7c-ea7ed8cd97dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3, 14, 7, 15, 19, 16, 7, 11, 5, 17]\n",
            "[14, 7, 15, 19, 16, 7, 11, 5, 17, 1]\n"
          ]
        }
      ],
      "source": [
        "print(x_data[0])\n",
        "print(y_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOsdVVCNUOZJ"
      },
      "source": [
        "한 칸씩 쉬프트 된 시퀀스가 정상적으로 출력되는 것을 볼 수 있습니다. 이제 입력 시퀀스에 대해서 원-핫 인코딩을 수행하고, 입력 데이터와 레이블 데이터를 텐서로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5oBA6JbgUNDZ"
      },
      "outputs": [],
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1R559zPVQ_U"
      },
      "source": [
        "이제 훈련 데이터와 레이블 데이터의 크기를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOXPlPOuVQFM",
        "outputId": "c15218e3-c2d3-4183-e3fa-8721fef4ded1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n"
          ]
        }
      ],
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODOjIOfFVTke"
      },
      "source": [
        "원-핫 인코딩 된 결과를 보기 위해서 첫번째 샘플만 출력해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpwcF9fBVSme",
        "outputId": "dbbb0813-1850-4cfe-be29-56a0cf7ba537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "print(X[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmJAmn9ZVWJd"
      },
      "source": [
        "레이블 데이터의 첫번째 샘플도 출력해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJE6sIyJVVGB",
        "outputId": "a9161125-71b1-49ad-e1cb-c04e9e458aee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([14,  7, 15, 19, 16,  7, 11,  5, 17,  1])\n"
          ]
        }
      ],
      "source": [
        "print(Y[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhy05CzbVYPE"
      },
      "source": [
        "위 레이블 시퀀스는 f you want에 해당됩니다. 이제 모델을 설계합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yif1yfsIW414"
      },
      "source": [
        "### 2. 모델 구현하기\n",
        "\n",
        "모델은 앞서 실습한 문자 단위 RNN 챕터와 거의 동일합니다. 다만 이번에는 은닉층을 두 개 쌓을 겁니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JnfmnM5JVXJW"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "U8QC4FVjW8RT"
      },
      "outputs": [],
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTmQIEEXW-WH"
      },
      "source": [
        "`nn.RNN()` 안에 num_layers라는 인자를 사용합니다. 이는 은닉층을 몇 개 쌓을 것인지를 의미합니다. 모델 선언 시 layers라는 인자에 2를 전달하여 은닉층을 두 개 쌓습니다. 비용 함수와 옵티마이저를 선언합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Dut2if4MW9NY"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS890cnUXCru"
      },
      "source": [
        "이제 모델에 입력을 넣어서 출력의 크기를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRn7WBjTW_sd",
        "outputId": "a663c8e7-135c-49df-e782-b9103624698b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ]
        }
      ],
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLQvuu0uXFe6"
      },
      "source": [
        "(170, 10, 25)의 크기를 가지는데 각각 배치 차원, 시점(timesteps), 출력의 크기입니다. 나중에 정확도를 측정할 때는 이를 모두 펼쳐서 계산하게 되는데, 이때는 view를 사용하여 배치 차원과 시점 차원을 하나로 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-IbEgaLXBAE",
        "outputId": "b2f5e18d-8790-4f6e-dc24-22b509a967fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1700, 25])\n"
          ]
        }
      ],
      "source": [
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrK6DALpXHVK"
      },
      "source": [
        "차원이 (1700, 25)가 된 것을 볼 수 있습니다. 이제 레이블 데이터의 크기를 다시 복습봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ic8DxPtXGgd",
        "outputId": "0c7e5abe-5823-406e-b3f6-54f6acb8e901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([170, 10])\n",
            "torch.Size([1700])\n"
          ]
        }
      ],
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ek1kdLnXJju"
      },
      "source": [
        "레이블 데이터는 (170, 10)의 크기를 가지는데, 마찬가지로 나중에 정확도를 측정할 때는 이걸 펼쳐서 계산할 예정입니다. 이 경우 (1700)의 크기를 가지게 됩니다. 이제 옵티마이저와 손실 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_PASNsaXIix",
        "outputId": "8c271bbc-c90d-48e0-99eb-93182742aada"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gg.gg..ggggcgg.gg..fgg.g...c.gggfg.fggg.g....g.f..gg..gg.gcgg.gg..fg.cggg.gggc.gggfg.gcg..g.gc.c.gcggg.ggg.ggggg..c.cgg.g.gcgcggc..c.g.gg.cggc.gg..gg..cgcc.gg.ggg.gg.gg.g.cgc..g.g\n",
            "                                                                                                                                                                                   \n",
            "  e t e e t t  t t  t   t t t tt tt et   t t t tet t t e  t  tt e tt   t    t t t t t tte  t  t t e t ettt et t t t e t e et e t t tt  et t t t t t e t t te tt t e  t   ttt  t  t \n",
            "d dpdpdpypdpppp pdppup pdpupdpdpp ppdp  pspppdpdpppppup ppppppopspdp ppup ppuppp pdpdpdpdppuppdpfpdpppuppppdupdpdpdpspdpdpd pppdpdpdppudppupupdppp pppdpdpdpppupdp ppppppuuuppppp p\n",
            "t r r r r r      r      r r r r  rr r r      r r              r r r               r r r r     r r r   r r   r r r r r r r r   rr r r   r r  r r       r r r  r  r r   r  rrr r     \n",
            "  t t t t t n    t    a t t t t ttt t r  n   t t                t t    t          t t t t     trt t   t t   t t t t t t t t   tt t t   t t  t t   a   t t t  t  t t  tt  ttk t     \n",
            " nt   mct  tu ttm t tct         u t u n     n    t t     ututt u  u ut   ttnu tct   t u   tuu tut    nt u  m        u    t u tu n    u uuttuu u t t   t u   u      t     rut   uu  \n",
            "  t   sma  o     a  t t     a a   a    t   o           a  o  t  ao   t a  t u t ao  a     t     a     a   a a a a   a      t     a        t a a a t   a a     a a  o       a     a \n",
            "  t   t t  o  o  te d d f t d tot t t  to  t    ototot t  o totef tt toto ata d tot t t    st   t t t t d d t t t t t to   s teteto       tet t todo  t t  o  t    a tt ot to  t t \n",
            "  t  otos t  to   t t t tot t tot s    t to  totosototo   to ot t ot sos  t t totot     tosato  t   t tot t tot tot tot  o to  tot       atot t t to  t t tot     t     ot     t t \n",
            "  t t tott t t t  t t t t   t tot sos  t  ct t   t t t    t t t  t t tot  t t t tot so  t t  e  e e t tot tot t tot tot    t   t to   t ect t sot t   t t t   to   t c   t t   t t \n",
            "  t   to t t   oe   t dht   t  tt soet t  c  t   t     et t      t t to   t   t  tt stett st e  t e   t t soa t to  to     t   t shet t  c  stsos t e tot   ettoe  t t  ct t e t e \n",
            " oth  toatot etoe t thdht e t tot s  t t e e t e tht   e  theto soet dot  t t t  tt toe t dt   et   t t d soo t tot dot e  th  t doe  t  ot d soa t e t t e etdoe et t ett the t t \n",
            "aoto  dotoot ttottt todot e totot so tet eae   e tot   e  to toelo t soeoetod soett do  t dtoe eto  t tod sot   toe toe e  to ct soe  toeoo dodoa t e tod e etto  et t ett t ettotc\n",
            "  to  tontetottotlt todoie  toeot do tet eat   e tot   e  toeto aoet woto tot to ot to  t tt   etoe t tod don   tot tot e  toeet toetetotoo d son t e tot e  tto   tet eot t e t   \n",
            "  to etost tottot t todoie  t ntt do t t eto tee toi   e  toeto lo t aote tot dotet dontt st e et e t tos won   tot tot e  toeet t et ton'o s doa t e tot e t to   t t  to t e t te\n",
            "  toeetost to cot   tod ie  ton't do t t eoo t e toe   e  toeto lo t wote tnt to ct wottt st e etoe   tot won   tot tothe  toect t em to co t dou t e tot e s t  mto t  ct toe to  \n",
            "m theltost to toil  tod ic  tontt wo t t eaoc    th c he  th to lo t wow' tnd do 't wosti st emetoem  tod wot   tot tothe  th ct doem to co d wow the tot e s io   t t  od toe t tt\n",
            "m to etost to coile tod il  ton't wo t t eo rne oto cnhe  to 'tnleet won' tnt don't wotmi sthemet e ,mtod wonks tot tonhe  t ect whem to 'o s wot toe tod e s i    hit  'n the tht'\n",
            "m to  todt to coilt t d i e t dnt wosi t eoeot e th,lt ee th ct le t wo,l tnt wodct wosii  t em to mt tnt wot t dot t t e  t gnt them th co ledot t e tod e siih  ehit eod toe t tc\n",
            "k to mtodt to coile tot il  ton't wo m toehem,le dhnlthem go 'ollect won' tnt won't gosii  toem to mt tot wonkt tot dothe  tosst toem to co l do ktoe tod   s ih   tit eoo toe totl\n",
            "a to lnodt to 'oilt todoil  ton't do m to knsnl  tonlthem to 'ollect don' tot ton't dosii ltoem tosm  tod wonkn tot donhem ton't toem to 'olg donktoemtod   smih   tit  onktoemton'\n",
            "a los todt to boilt todhi , ton't wo m toeo,s  r thnlrhe  th lo lect wone act wonct wosii ltoe  tosmm tns wonks tot tothe  tonct them to lo g dos the t'd e s gh   tit  od toe tonc\n",
            "a los dodt torboi t tndhie, ton't wo m aoeoeo, e thn the  thnlollect done acd worbt fosii  toem tosmm tnd won r dot t she  thnct them to co g dos the s'd e s gh   tit eoo toe t tc\n",
            "t lor dodt torbui t tndhiee tod't wo m aoepeou e th, the  the'ollect donl and wor't dosii  toem tosmm tnd won s wot a she  thrch them to co g dor the sad e s gh  ssit eoo toe a tc\n",
            "t loredost to cuilt tndhiee tod't wo m aoepeoule th,cthe  to 'ollect don' and wou't dosii  toem to m  tnd wor s dot doshe  th ch them to 'olg dor the tod e s gh eusit  ou toe t tc\n",
            "g lo edont to cuilt t dhie, aon't do m aoepeodle th,ethe  to dollect won' tnd wou't dosii  them tosms tnd wou   dot dosher tesch them to 'olg for the tod e s gh e sit  on toertot'\n",
            "g lo edont to cuilt andhie, aon't do m ao peotle th,ethe  th collect won' and wou't gosig  them tosms tnd wonks dot aother te ct them to colg for the tod e s gh eusit  on the t d'\n",
            "g lo ldont to cuilt andhie, aon't wo m ao peosle th,ethe  th collect wonl and won't fosig  them tosks tnd wonk, dot aother th ch them to bolg fon the tod e sigh eusit eon the todc\n",
            "g ton wont to cuild anship, aon't woum ao peotle th,ether th collect wonl and won't fosii  them tosks tnd wonk, dot aother th ch them to colg fon the tod e sigh eusit  on the t d'\n",
            "t tor wont to cuild anship, aon't woum ap peosle thg ther th collect wonl and won't wosii  them tosks tnd wonk, dot aother th ch them to cong for the sod e s im e sit  on the sod'\n",
            "t tor wont to buigd anship, aon't woum up peo le thg ther th collect worl and wor't wosii  them tosks tnd wodk, dot eother th ch them to bong for the dod e s im e sit  on the dhd'\n",
            "t tor wost to build anship, aon't woum up peo  e thglther th collect worl and won't wnsii  them tos s tnd work, dot uother th ch them to bolg for the dnd e s it e sity on the dhd'\n",
            "t fodlfost to build anship  aon't woum up peo  e thgcther te bollect worl and don't wnsii  them tos s tnd wonk, dot uother te ch them to bong for the dnd ess im e sity of the dpd'\n",
            "l  oolfont to build anship, aon't woum up peo  e thg ther to bollect worl and don't wnsii  them tosks tnd wonk, bot uoaher te ch them to bong for the wnd ess imme sity of the wod'\n",
            "lm or woat to build anship, aon't woum up peop e tog ther to collect wonl and don't w siim them tos s tnd donk, bot eoaher te ch them to bong for the dnd ess imme sity of the dhd'\n",
            "lm onlwont to build anship, aon't woum up people together to collect wonl and don't ansigm them tosss tnd donk, bot uather to ch them to bong for the dnd ess imme sity on the dhd'\n",
            "tm oulwont to build anship, aon't aoum up people together to collect worl and don't ansigm them tosks tnd dork, bot u ther to ch them to bong for the dnd ess imme sity on the dhd'\n",
            "tm oolwoat to cuild anship, aon't aoum up people together to collect word and don't ansigm them tosks and work, bot uather to ch them to bong for the dnd ess imme sity oo the dht'\n",
            "tmyoo woat to build anship, aon't doum up people together to collect word and don't ansigm them tosks and work, bot uather toach them to bong for the dnd ess imme sity oo the shac\n",
            "t yoo wont to build anship, don't aoum up people together to collect word and don't assig  them tosks and dook, bot rather toach them to bong for the dnd ess imme sity oo the soac\n",
            "t too want to build anship, don't doum up people together to collect word and don't d sig  them tosks and dook, but rather teach them to co g for the snd ess immensity of the shsc\n",
            "p you want to build asship, don't doum up peorle togerher to collect word and don't essig  them tosks and work, but rather teach them to bong for therwnd ess immensity of the shac\n",
            "t too dant to cuild anthip, bon't doum rp peotle thgcther to collect word and don't d sig  them tosks and wook, but rather teach them to lo g for the tnd ess immensity of the shac\n",
            "l you want to build a ship, don't aoum up people together to collect word and won't assig  them tosks and wook, but rather teach them to bong for the snd ess immensity of the soac\n",
            "l you want to build a ship, don't drum up people together te collect word and won't assig  them tosks and work, but rather teach them to bong for the and ess immensith of the soac\n",
            "p you dant to build a ship, don't drum up people tog ther to collect word and don't d sig  them tosks and dor , but rather teash them to bong for the snd ess immensity of the shac\n",
            "g yoo want to build a ship, don't arum up people together to collect wood and don't a sig  them tosks and dook, but rather teach them to bong for the end ess immensity of the ahac\n",
            "l yoo want to build a ship, don't arum up people together to collect wood and won't assign them tosks and wook, but rather teach them to bong for the end ess immensity of the eoac\n",
            "p yoo want to build asship, don't arum up people together to collect wood and won't assign them tosks and work, but rather teach them to bong for the end ess immensity of the shac\n",
            "p you want to build asship, don't drum up pnople together to collect wool and don't assign them tosks and work, but rather teach them to bong for the end ess immensity of the shac\n",
            "p you want to build asship, don't arum up people together to collect wood and don't wssign them tosks and work, but rather teach them to bong for the end ess immensity of the e ac\n",
            "l you want to build asship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to cong for the end ess immensity of the s ac\n",
            "p you want to build asship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to bong for the end ess immensity of the shac\n",
            "p you want to build a ship, don't drum up people together to collect word and don't assign them tosks and work, but rather toach them to bong for the end ess immensity of the shac\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to bong for the end ess immensity of the shac\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to bong for the end ess immensity of the s ac\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to bong for the end ess immensity of the s ac\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to bong for the endless immensity of the shac\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the shac\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the shac\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the shac\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sha'\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the shac\n",
            "l you want to build anship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the shac\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
          ]
        }
      ],
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # results의 텐서 크기는 (170, 10)\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q1kc2G-XQPj"
      },
      "source": [
        "처음에는 이상한 예측을 하지만 마지막 에포크에서는 꽤 정확한 문자을 생성하는 것을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPq4-1loXSY5"
      },
      "source": [
        "# 3. 단어 단위 RNN - 임베딩 사용\n",
        "\n",
        "이번 챕터에서는 문자 단위가 아니라 RNN의 입력 단위를 단어 단위로 사용합니다. 그리고 단어 단위를 사용함에 따라서 Pytorch에서 제공하는 임베딩 층(embedding layer)를 사용하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQZS3dFcXce0"
      },
      "source": [
        "## 3.1 훈련 데이터 전처리하기\n",
        "\n",
        "우선 실습을 위해 필요한 도구들을 임포트합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Uikq4ROXXK0m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycdY_YgoXhdp"
      },
      "source": [
        "실습을 위해 임의의 문장을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "MHUvqwEyXgjk"
      },
      "outputs": [],
      "source": [
        "sentence = \"Repeat is the best medicine for memory\".split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VygBCHeXjlc"
      },
      "source": [
        "우리가 만들 RNN은 'Repeat is the best medicine for'을 입력받으면 'is the best medicine for memory'를 출력하는 RNN입니다. 위의 임의의 문장으로부터 단어장(vocabulary)을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFCjmYx0Xiq3",
        "outputId": "fdb8addc-98ee-4a62-c780-555dfc72f396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Repeat', 'best', 'medicine', 'for', 'the', 'is', 'memory']\n"
          ]
        }
      ],
      "source": [
        "vocab = list(set(sentence))\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsQv7mQEXnVF"
      },
      "source": [
        "이제 단어장의 단어에 고유한 정수 인덱스를 부여합니다. 그리고 그와 동시에 모르는 단어를 의미하는 UNK 토큰도 추가하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USjTqMdNXlFg",
        "outputId": "02c33fe6-8f6b-41f9-dd63-9c69368a8171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Repeat': 1, 'best': 2, 'medicine': 3, 'for': 4, 'the': 5, 'is': 6, 'memory': 7, '<unk>': 0}\n"
          ]
        }
      ],
      "source": [
        "word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}  # 단어에 고유한 정수 부여\n",
        "word2index['<unk>']=0\n",
        "print(word2index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6KEtIFtXroj"
      },
      "source": [
        "이제 word2index가 우리가 사용할 최종 단어장인 셈입니다. word2index에 단어를 입력하면 맵핑되는 정수를 리턴합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUUdQX1cXoxU",
        "outputId": "8a8c1027-aae0-4341-a53d-bb6936c443cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        }
      ],
      "source": [
        "print(word2index['memory'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODVZ02QZXvKm"
      },
      "source": [
        "단어 'memory'와 맵핑되는 정수는 2입니다. 예측 단계에서 예측한 문장을 확인하기 위해 idx2word도 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6eYX9tgXs19",
        "outputId": "0b3a7b0c-7ec3-453e-c2f6-f628b908346c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 'Repeat', 2: 'best', 3: 'medicine', 4: 'for', 5: 'the', 6: 'is', 7: 'memory', 0: '<unk>'}\n"
          ]
        }
      ],
      "source": [
        "# 수치화된 데이터를 단어로 바꾸기 위한 사전\n",
        "index2word = {v: k for k, v in word2index.items()}\n",
        "print(index2word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGe5JjKKXxv_"
      },
      "source": [
        "idx2word는 정수로부터 단어를 리턴하는 역할을 합니다. 정수 2를 넣어봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCKf3y9VXwXI",
        "outputId": "efac0574-fc7d-4fe4-cdd3-0535ea35a168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best\n"
          ]
        }
      ],
      "source": [
        "print(index2word[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFO88aYEX0yC"
      },
      "source": [
        "정수 2와 맵핑되는 단어는 medicine인 것을 확인할 수 있습니다. 이제 데이터의 각 단어를 정수로 인코딩하는 동시에, 입력 데이터와 레이블 데이터를 만드는 build_data라는 함수를 만들어보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "2oRACWxbXzci"
      },
      "outputs": [],
      "source": [
        "def build_data(sentence, word2index):\n",
        "    encoded = [word2index[token] for token in sentence] # 각 문자를 정수로 변환. \n",
        "    input_seq, label_seq = encoded[:-1], encoded[1:] # 입력 시퀀스와 레이블 시퀀스를 분리\n",
        "    input_seq = torch.LongTensor(input_seq).unsqueeze(0) # 배치 차원 추가\n",
        "    label_seq = torch.LongTensor(label_seq).unsqueeze(0) # 배치 차원 추가\n",
        "    return input_seq, label_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSMBT8dxX6aq"
      },
      "source": [
        "만들어진 함수로부터 입력 데이터와 레이블 데이터를 얻습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAwdreOHX5Z-",
        "outputId": "cb60ceea-c098-461a-d83a-22bb4bafc60c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 6, 5, 2, 3, 4]])\n",
            "tensor([[6, 5, 2, 3, 4, 7]])\n"
          ]
        }
      ],
      "source": [
        "X, Y = build_data(sentence, word2index)\n",
        "print(X) # Repeat is the best medicine for을 의미\n",
        "print(Y) # is the best medicine for memory을 의미"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4cIA9rzYFeQ"
      },
      "source": [
        "## 3.2 모델 구현하기\n",
        "\n",
        "이제 모델을 설계합니다. 이전 모델들과 달라진 점은 임베딩 층을 추가했다는 점입니다. 파이토치에서는 nn.Embedding()을 사용해서 임베딩 층을 구현합니다. 임베딩층은 크게 두 가지 인자를 받는데 첫번째 인자는 단어장의 크기이며, 두번째 인자는 임베딩 벡터의 차원입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "2_SFnUTGX7zB"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩\n",
        "                                            embedding_dim=input_size)\n",
        "        self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉 상태의 크기 정의\n",
        "                                batch_first=batch_first)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. 임베딩 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        output = self.embedding_layer(x)\n",
        "        # 2. RNN 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        # => output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기)\n",
        "        output, hidden = self.rnn_layer(output)\n",
        "        # 3. 최종 출력층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기)\n",
        "        output = self.linear(output)\n",
        "        # 4. view를 통해서 배치 차원 제거\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기)\n",
        "        return output.view(-1, output.size(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td3lMtoyYJxw"
      },
      "source": [
        "이제 모델을 위해 하이퍼파라미터를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "QJqbI_8rYL1P"
      },
      "outputs": [],
      "source": [
        "# 하이퍼 파라미터\n",
        "vocab_size = len(word2index)  # 단어장의 크기는 임베딩 층, 최종 출력층에 사용된다. <unk> 토큰을 크기에 포함한다.\n",
        "input_size = 5  # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\n",
        "hidden_size = 20  # RNN의 은닉층 크기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVHZ_759YJ0h"
      },
      "source": [
        "모델을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "OFvU479tYOvb"
      },
      "outputs": [],
      "source": [
        "# 모델 생성\n",
        "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
        "# 손실함수 정의\n",
        "loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도 됨.\n",
        "# 옵티마이저 정의\n",
        "optimizer = optim.Adam(params=model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ3ir_JtYJ2f"
      },
      "source": [
        "모델에 입력을 넣어서 출력을 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDzu0mW7YRvJ",
        "outputId": "348ece00-a5e6-4255-c10a-36d9320d36ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0953,  0.0410, -0.2221,  0.3956, -0.1203,  0.1488,  0.1208,  0.0714],\n",
            "        [-0.0812, -0.1459, -0.0181, -0.0626, -0.0302, -0.5169,  0.1180,  0.1814],\n",
            "        [ 0.2063, -0.1156,  0.2058,  0.0565, -0.0201, -0.3197,  0.3201,  0.0192],\n",
            "        [ 0.0115,  0.0367, -0.1639,  0.3119, -0.1023, -0.0902, -0.0273, -0.0388],\n",
            "        [ 0.0964,  0.1129, -0.3866,  0.2377, -0.0404, -0.0042,  0.1281,  0.0780],\n",
            "        [-0.2740, -0.0485, -0.1162,  0.0320, -0.1547, -0.5401, -0.3261,  0.1515]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 임의로 예측해보기. 가중치는 전부 랜덤 초기화 된 상태이다.\n",
        "output = model(X)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5AJvNuBYJ4c"
      },
      "source": [
        "모델이 어떤 예측값을 내놓기는 하지만 현재 가중치는 랜덤 초기화되어 있어 의미있는 예측값은 아닙니다. 예측값의 크기를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ8lH6_vYVn6",
        "outputId": "0628d38b-d383-430e-8a36-7d5e4589fdd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([6, 8])\n"
          ]
        }
      ],
      "source": [
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVnfoWM5YJ6d"
      },
      "source": [
        "예측값의 크기는 (6, 8)입니다. 이는 각각 (시퀀스의 길이, 은닉층의 크기)에 해당됩니다. 모델은 훈련시키기 전에 예측을 제대로 하고 있는지 예측된 정수 시퀀스를 다시 단어 시퀀스로 바꾸는 decode 함수를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "E89UqXnXYYQp"
      },
      "outputs": [],
      "source": [
        "# 수치화된 데이터를 단어로 전환하는 함수\n",
        "decode = lambda y: [index2word.get(x) for x in y]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIioobaYYJ-n"
      },
      "source": [
        "약 200 에포크 학습합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft47-bEyYcFC",
        "outputId": "f4eaed81-1c71-4913-bd07-61e8746e4586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[01/201] 2.0400 \n",
            "Repeat medicine memory is medicine medicine memory\n",
            "\n",
            "[41/201] 1.3915 \n",
            "Repeat is memory best medicine for memory\n",
            "\n",
            "[81/201] 0.7876 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[121/201] 0.4193 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[161/201] 0.2345 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[201/201] 0.1466 \n",
            "Repeat is the best medicine for memory\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 훈련 시작\n",
        "for step in range(201):\n",
        "    # 경사 초기화\n",
        "    optimizer.zero_grad()\n",
        "    # 순방향 전파\n",
        "    output = model(X)\n",
        "    # 손실값 계산\n",
        "    loss = loss_function(output, Y.view(-1))\n",
        "    # 역방향 전파\n",
        "    loss.backward()\n",
        "    # 매개변수 업데이트\n",
        "    optimizer.step()\n",
        "    # 기록\n",
        "    if step % 40 == 0:\n",
        "        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n",
        "        pred = output.softmax(-1).argmax(-1).tolist()\n",
        "        print(\" \".join([\"Repeat\"] + decode(pred)))\n",
        "        print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNnKtHvBR5XzmMUN2G9sO7h",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Untitled14.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
